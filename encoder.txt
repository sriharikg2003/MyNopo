(Pdb) encoder
EncoderNoPoSplat(
  (backbone): AsymmetricCroCo(
    (patch_embed): PatchEmbedDust3R(
      (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
      (norm): Identity()
    )
    (mask_generator): RandomMask()
    (rope): RoPE2D()
    (enc_blocks): ModuleList(
      (0-23): 24 x Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (rope): RoPE2D()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (enc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (decoder_embed): Linear(in_features=1024, out_features=768, bias=True)
    (dec_blocks): ModuleList(
      (0-11): 12 x DecoderBlock(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (rope): RoPE2D()
        )
        (cross_attn): CrossAttention(
          (projq): Linear(in_features=768, out_features=768, bias=True)
          (projk): Linear(in_features=768, out_features=768, bias=True)
          (projv): Linear(in_features=768, out_features=768, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (rope): RoPE2D()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (norm_y): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
    )
    (dec_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (dec_blocks2): ModuleList(
      (0-11): 12 x DecoderBlock(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (rope): RoPE2D()
        )
        (cross_attn): CrossAttention(
          (projq): Linear(in_features=768, out_features=768, bias=True)
          (projk): Linear(in_features=768, out_features=768, bias=True)
          (projv): Linear(in_features=768, out_features=768, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (rope): RoPE2D()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (norm_y): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
    )
    (intrinsic_encoder): Linear(in_features=9, out_features=1024, bias=True)
  )
  (gaussian_adapter): UnifiedGaussianAdapter()
  (downstream_head1): PixelwiseTaskWithDPT(
    (dpt): DPTOutputAdapter_fix(
      (scratch): Module(
        (layer1_rn): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer2_rn): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer3_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer4_rn): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer_rn): ModuleList(
          (0): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (3): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (5): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (6): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (7): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
        (refinenet1): FeatureFusionBlock_custom(
          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet2): FeatureFusionBlock_custom(
          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet3): FeatureFusionBlock_custom(
          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet4): FeatureFusionBlock_custom(
          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
      )
      (head): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): Interpolate()
        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
        (4): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (act_postprocess): ModuleList(
        (0): Sequential(
          (0): Conv2d(1024, 96, kernel_size=(1, 1), stride=(1, 1))
          (1): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(4, 4))
        )
        (1): Sequential(
          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
          (1): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))
        )
        (2): Sequential(
          (0): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
        )
        (3): Sequential(
          (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
        (4): Sequential(
          (0): Conv2d(1024, 96, kernel_size=(1, 1), stride=(1, 1))
          (1): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(4, 4))
        )
        (5): Sequential(
          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
          (1): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))
        )
        (6): Sequential(
          (0): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
        )
        (7): Sequential(
          (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
      )
    )
  )
  (downstream_head2): PixelwiseTaskWithDPT(
    (dpt): DPTOutputAdapter_fix(
      (scratch): Module(
        (layer1_rn): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer2_rn): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer3_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer4_rn): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer_rn): ModuleList(
          (0): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (3): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (5): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (6): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (7): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
        (refinenet1): FeatureFusionBlock_custom(
          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet2): FeatureFusionBlock_custom(
          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet3): FeatureFusionBlock_custom(
          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet4): FeatureFusionBlock_custom(
          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
      )
      (head): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): Interpolate()
        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
        (4): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1))
      )
      (act_postprocess): ModuleList(
        (0): Sequential(
          (0): Conv2d(1024, 96, kernel_size=(1, 1), stride=(1, 1))
          (1): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(4, 4))
        )
        (1): Sequential(
          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
          (1): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))
        )
        (2): Sequential(
          (0): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
        )
        (3): Sequential(
          (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
        (4): Sequential(
          (0): Conv2d(1024, 96, kernel_size=(1, 1), stride=(1, 1))
          (1): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(4, 4))
        )
        (5): Sequential(
          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
          (1): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))
        )
        (6): Sequential(
          (0): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
        )
        (7): Sequential(
          (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
      )
    )
  )
  (gaussian_param_head): PixelwiseTaskWithDPT(
    (dpt): DPTOutputAdapter_fix(
      (scratch): Module(
        (layer1_rn): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer2_rn): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer3_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer4_rn): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer_rn): ModuleList(
          (0): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (3): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (5): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (6): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (7): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
        (refinenet1): FeatureFusionBlock_custom(
          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet2): FeatureFusionBlock_custom(
          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet3): FeatureFusionBlock_custom(
          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet4): FeatureFusionBlock_custom(
          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
      )
      (head): Sequential(
        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): Identity()
        (2): ReLU(inplace=True)
        (3): Dropout(p=0.1, inplace=False)
        (4): Conv2d(256, 83, kernel_size=(1, 1), stride=(1, 1))
      )
      (act_postprocess): ModuleList(
        (0): Sequential(
          (0): Conv2d(1024, 96, kernel_size=(1, 1), stride=(1, 1))
          (1): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(4, 4))
        )
        (1): Sequential(
          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
          (1): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))
        )
        (2): Sequential(
          (0): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
        )
        (3): Sequential(
          (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
        (4): Sequential(
          (0): Conv2d(1024, 96, kernel_size=(1, 1), stride=(1, 1))
          (1): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(4, 4))
        )
        (5): Sequential(
          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
          (1): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))
        )
        (6): Sequential(
          (0): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
        )
        (7): Sequential(
          (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
      )
      (feat_up): Interpolate()
      (input_merger): Sequential(
        (0): Conv2d(3, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (1): ReLU()
      )
    )
  )
  (gaussian_param_head2): PixelwiseTaskWithDPT(
    (dpt): DPTOutputAdapter_fix(
      (scratch): Module(
        (layer1_rn): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer2_rn): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer3_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer4_rn): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer_rn): ModuleList(
          (0): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (3): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (5): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (6): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (7): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
        (refinenet1): FeatureFusionBlock_custom(
          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet2): FeatureFusionBlock_custom(
          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet3): FeatureFusionBlock_custom(
          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet4): FeatureFusionBlock_custom(
          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit_custom(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
      )
      (head): Sequential(
        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): Identity()
        (2): ReLU(inplace=True)
        (3): Dropout(p=0.1, inplace=False)
        (4): Conv2d(256, 83, kernel_size=(1, 1), stride=(1, 1))
      )
      (act_postprocess): ModuleList(
        (0): Sequential(
          (0): Conv2d(1024, 96, kernel_size=(1, 1), stride=(1, 1))
          (1): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(4, 4))
        )
        (1): Sequential(
          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
          (1): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))
        )
        (2): Sequential(
          (0): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
        )
        (3): Sequential(
          (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
        (4): Sequential(
          (0): Conv2d(1024, 96, kernel_size=(1, 1), stride=(1, 1))
          (1): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(4, 4))
        )
        (5): Sequential(
          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
          (1): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))
        )
        (6): Sequential(
          (0): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
        )
        (7): Sequential(
          (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
      )
      (feat_up): Interpolate()
      (input_merger): Sequential(
        (0): Conv2d(3, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (1): ReLU()
      )
    )
  )
)